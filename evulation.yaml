name: Inference and Anomaly Detection
description: Loads a trained STGNN model, runs inference on a specific data point, and performs anomaly detection with root cause analysis.

inputs:
  - name: trained_model
    type: Model
    description: The trained STGNN model from train_model brick

  - name: data_pth
    type: String
    description: Path to dataset file or folder from load_dataset brick

  - name: config_updated
    type: String
    description: Updated configuration parameters from build_model brick

outputs:
  - name: metrics
    type: Metric
    description: Evaluation metrics including anomaly score, detection result, and root cause contributions

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        # Install dependencies
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet torch torchvision torchaudio torch_geometric numpy --user

        python3 -u - <<'PYCODE'
        import argparse
        import json
        import torch
        import numpy as np
        import os
        from stgnn import stgnn
        from anomaly_dd import anomaly_dd
        from load_dataset import load_dataset

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--data_pth', type=str, required=True)
        parser.add_argument('--config_updated', type=str, required=True)
        parser.add_argument('--metrics', type=str, required=True)
        args = parser.parse_args()

        # Load config
        with open(args.config_updated, "r") as f:
            config = json.load(f)

        device = torch.device(config.get("device", "cpu"))

        # Load data
        dataloader = load_dataset(args.data_pth, config["batch_size"], config["batch_size"], config["batch_size"], config["scaling_required"])
        scaler = dataloader['scaler']

        # Load model
        model = stgnn(
            config["gcn_true"], config["buildA_true"], config["gcn_depth"], config["num_nodes"],
            device, predefined_A=None,
            dropout=config["dropout"], subgraph_size=config["subgraph_size"],
            node_dim=config["node_dim"], dilation_exponential=config["dilation_exponential"],
            conv_channels=config["conv_channels"], residual_channels=config["residual_channels"],
            skip_channels=config["skip_channels"], end_channels=config["end_channels"],
            seq_length=config["seq_in_len"], in_dim=config["in_dim"], out_dim=config["seq_out_len"],
            layers=config["layers"], propalpha=config["propalpha"], tanhalpha=config["tanhalpha"], layer_norm_affline=True
        )

        model.load_state_dict(torch.load(args.trained_model))
        model.to(device)
        model.eval()

        # Pick anomaly index
        anomaly_index = 3919
        x_test = dataloader['x_test']
        y_test = dataloader['y_test']

        if len(x_test) <= anomaly_index:
            raise ValueError(f"Test set has less than {anomaly_index + 1} data points.")

        single_x = torch.Tensor(x_test[anomaly_index:anomaly_index+1]).to(device).transpose(1, 3)
        single_y_true = y_test[anomaly_index:anomaly_index+1].squeeze()

        with torch.no_grad():
            pred_y, _ = model(single_x)
            pred_y = pred_y.transpose(1, 3)

        if config["scaling_required"]:
            pred_y = scaler.inverse_transform(pred_y)

        pred_y = pred_y.squeeze().cpu().detach().numpy()

        # Validation predictions
        val_outputs = []
        for _, (x, y) in enumerate(dataloader['val_loader'].get_iterator()):
            testx = torch.Tensor(x).to(device).transpose(1, 3)
            with torch.no_grad():
                preds, _ = model(testx)
            val_outputs.append(preds.transpose(1, 3))

        val_yhat = torch.cat(val_outputs, dim=0)
        val_realy = torch.Tensor(dataloader['y_val']).to(device)
        val_yhat = val_yhat[:val_realy.size(0), ...]

        if config["scaling_required"]:
            val_pred = scaler.inverse_transform(val_yhat)
        else:
            val_pred = val_yhat

        val_pred = val_pred.squeeze().cpu().detach().numpy()
        val_label = val_realy.squeeze().cpu().detach().numpy()

        # Anomaly detection
        anomaly_detector = anomaly_dd(
            train_obs=np.array([]).reshape(0, val_label.shape[1]), val_obs=val_label, test_obs=np.array([single_y_true]),
            train_forecast=np.array([]).reshape(0, val_pred.shape[1]), val_forecast=val_pred, test_forecast=np.array([pred_y]),
            window_length=config["normalization_window"], root_cause=True
        )

        indicator, prediction = anomaly_detector.scorer(config["pca_compo"])

        metrics_dict = {
            "anomaly_score": float(indicator[0]),
            "is_anomaly": bool(prediction[0]),
            "root_cause_contributions": {}
        }

        if prediction[0]:
            error_contribution = anomaly_detector.test_re_full[0]
            sorted_indices = np.argsort(error_contribution)[::-1]
            for i in range(min(3, len(sorted_indices))):
                param_index = sorted_indices[i]
                metrics_dict["root_cause_contributions"][str(param_index)] = float(error_contribution[param_index])

        # Save metrics
        import json
        with open(args.metrics, "w") as f:
            json.dump(metrics_dict, f)

        print("Metrics:", metrics_dict)
        PYCODE
    args:
      - --trained_model
      - { inputValue: trained_model }
      - --data_pth
      - { inputValue: data_pth }
      - --config_updated
      - { inputValue: config_updated }
      - --metrics
      - { outputPath: metrics }
