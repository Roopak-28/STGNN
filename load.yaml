name: Load Server Machine Dataset
description: Downloads the Server Machine Dataset (SMD) from the OmniAnomaly GitHub repo, processes it, and saves it as a PyTorch dataset.

outputs:
  - {name: smd_data, type: Dataset}

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        apt-get update && apt-get install -y git unzip wget
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet torch torchvision torchaudio pandas scikit-learn || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet torch torchvision torchaudio pandas scikit-learn --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pickle
        import numpy as np
        import pandas as pd
        import torch
        from sklearn.preprocessing import StandardScaler

        parser = argparse.ArgumentParser()
        parser.add_argument('--smd_data', type=str, required=True)
        args = parser.parse_args()

        print("[INFO] Downloading Server Machine Dataset (SMD) from OmniAnomaly GitHub...")
        os.makedirs("/tmp/smd", exist_ok=True)
        os.system("git clone --depth=1 https://github.com/NetManAIOps/OmniAnomaly.git /tmp/omni")

        smd_path = "/tmp/omni/ServerMachineDataset"
        train_files = sorted([os.path.join(smd_path, "train", f) for f in os.listdir(os.path.join(smd_path, "train")) if f.endswith(".txt")])
        test_files = sorted([os.path.join(smd_path, "test", f) for f in os.listdir(os.path.join(smd_path, "test")) if f.endswith(".txt")])
        label_files = sorted([os.path.join(smd_path, "test_label", f) for f in os.listdir(os.path.join(smd_path, "test_label")) if f.endswith(".txt")])

        print(f"[INFO] Found {len(train_files)} train files, {len(test_files)} test files")

        # Load all train/test files into tensors
        def load_smd_file(path):
            df = pd.read_csv(path, header=None, sep=",")
            return df.values.astype(np.float32)

        train_data = [load_smd_file(f) for f in train_files]
        test_data = [load_smd_file(f) for f in test_files]
        test_labels = [load_smd_file(f) for f in label_files]

        # Normalize each machine's data
        scaler = StandardScaler()
        train_data = [scaler.fit_transform(x) for x in train_data]
        test_data = [scaler.transform(x) for x in test_data]

        # Convert to torch tensors
        train_tensors = [torch.tensor(x) for x in train_data]
        test_tensors = [torch.tensor(x) for x in test_data]
        label_tensors = [torch.tensor(x) for x in test_labels]

        dataset = {
            "train": train_tensors,
            "test": test_tensors,
            "labels": label_tensors,
            "scaler_mean": scaler.mean_.tolist(),
            "scaler_scale": scaler.scale_.tolist()
        }

        # Ensure output directory
        os.makedirs(os.path.dirname(args.smd_data), exist_ok=True)

        # Save dataset object
        with open(args.smd_data, "wb") as f:
            pickle.dump(dataset, f)

        print(f"[SUCCESS] Saved Server Machine Dataset to {args.smd_data}")

    args:
      - --smd_data
      - {outputPath: smd_data}
