name: Load Server Machine Dataset
description: Downloads the Server Machine Dataset (SMD) from the OmniAnomaly GitHub repo, processes it, and saves train/test/label CSV files into a shared PVC for downstream tasks.

outputs:
  - name: train_csv
    type: String
    description: Path to the training dataset CSV
  - name: test_csv
    type: String
    description: Path to the testing dataset CSV
  - name: labels_csv
    type: String
    description: Path to the anomaly labels CSV

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        pip install pandas numpy scikit-learn --quiet
        python3 -u - <<'EOF'
        import os
        import pandas as pd
        import numpy as np
        from sklearn.preprocessing import StandardScaler

        print("[INFO] Downloading Server Machine Dataset (SMD) from OmniAnomaly GitHub...")
        os.makedirs("/tmp/smd", exist_ok=True)
        os.system("git clone --depth=1 https://github.com/NetManAIOps/OmniAnomaly.git /tmp/omni")

        smd_path = "/tmp/omni/ServerMachineDataset"
        train_files = sorted([os.path.join(smd_path, "train", f) for f in os.listdir(os.path.join(smd_path, "train")) if f.endswith(".txt")])
        test_files = sorted([os.path.join(smd_path, "test", f) for f in os.listdir(os.path.join(smd_path, "test")) if f.endswith(".txt")])
        label_files = sorted([os.path.join(smd_path, "test_label", f) for f in os.listdir(os.path.join(smd_path, "test_label")) if f.endswith(".txt")])

        print(f"[INFO] Found {len(train_files)} train files, {len(test_files)} test files, {len(label_files)} label files")

        def load_smd_file(path):
            return pd.read_csv(path, header=None)

        train_dfs = [load_smd_file(f) for f in train_files]
        test_dfs = [load_smd_file(f) for f in test_files]
        label_dfs = [load_smd_file(f) for f in label_files]

        train_all = pd.concat(train_dfs, ignore_index=True)
        test_all = pd.concat(test_dfs, ignore_index=True)
        labels_all = pd.concat(label_dfs, ignore_index=True)

        scaler = StandardScaler()
        train_scaled = pd.DataFrame(scaler.fit_transform(train_all))
        test_scaled = pd.DataFrame(scaler.transform(test_all))

        # Use PVC mount path
        output_dir = "/mnt/shared/smd"
        os.makedirs(output_dir, exist_ok=True)

        train_path = os.path.join(output_dir, "train.csv")
        test_path = os.path.join(output_dir, "test.csv")
        labels_path = os.path.join(output_dir, "labels.csv")

        train_scaled.to_csv(train_path, index=False)
        test_scaled.to_csv(test_path, index=False)
        labels_all.to_csv(labels_path, index=False)

        # Write paths to stdout so Elyra/KFP registers them as outputs
        print(train_path)
        print(test_path)
        print(labels_path)
        EOF
    args:
      - --train_csv
      - /mnt/shared/smd/train.csv
      - --test_csv
      - /mnt/shared/smd/test.csv
      - --labels_csv
      - /mnt/shared/smd/labels.csv
