name: Load Server Machine Dataset
description: Downloads the Server Machine Dataset (SMD) from the OmniAnomaly GitHub repo, processes it, and saves train/test/label CSV files for downstream tasks.

outputs:
  - name: train_csv
    type: String
    description: Path to the training dataset CSV
  - name: test_csv
    type: String
    description: Path to the testing dataset CSV
  - name: labels_csv
    type: String
    description: Path to the anomaly labels CSV

implementation:
  container:
    image: python:3.9
    command:
      - python3
      - -u
      - -c
      - |
        import os
        # Ensure required dependencies are installed
        os.system("apt-get update && apt-get install -y git && pip install --no-cache-dir pandas numpy scikit-learn")

        import argparse
        import pandas as pd
        import numpy as np
        from sklearn.preprocessing import StandardScaler

        parser = argparse.ArgumentParser()
        parser.add_argument('--train_csv', type=str, required=True)
        parser.add_argument('--test_csv', type=str, required=True)
        parser.add_argument('--labels_csv', type=str, required=True)
        args = parser.parse_args()

        print("[INFO] Downloading Server Machine Dataset (SMD) from OmniAnomaly GitHub...")
        os.makedirs("/tmp/smd", exist_ok=True)
        os.system("git clone --depth=1 https://github.com/NetManAIOps/OmniAnomaly.git /tmp/omni")

        smd_path = "/tmp/omni/ServerMachineDataset"
        train_files = sorted([os.path.join(smd_path, "train", f) for f in os.listdir(os.path.join(smd_path, "train")) if f.endswith(".txt")])
        test_files = sorted([os.path.join(smd_path, "test", f) for f in os.listdir(os.path.join(smd_path, "test")) if f.endswith(".txt")])
        label_files = sorted([os.path.join(smd_path, "test_label", f) for f in os.listdir(os.path.join(smd_path, "test_label")) if f.endswith(".txt")])

        print(f"[INFO] Found {len(train_files)} train files, {len(test_files)} test files, {len(label_files)} label files")

        def load_smd_file(path):
            return pd.read_csv(path, header=None)

        train_dfs = [load_smd_file(f) for f in train_files]
        test_dfs = [load_smd_file(f) for f in test_files]
        label_dfs = [load_smd_file(f) for f in label_files]

        # Concatenate all machines together
        train_all = pd.concat(train_dfs, ignore_index=True)
        test_all = pd.concat(test_dfs, ignore_index=True)
        labels_all = pd.concat(label_dfs, ignore_index=True)

        # Normalize features
        scaler = StandardScaler()
        train_scaled = pd.DataFrame(scaler.fit_transform(train_all))
        test_scaled = pd.DataFrame(scaler.transform(test_all))

        # Ensure output directory
        os.makedirs(os.path.dirname(args.train_csv), exist_ok=True)

        # Save CSVs
        train_scaled.to_csv(args.train_csv, index=False)
        test_scaled.to_csv(args.test_csv, index=False)
        labels_all.to_csv(args.labels_csv, index=False)

        print(f"[SUCCESS] Train saved to {args.train_csv}")
        print(f"[SUCCESS] Test saved to {args.test_csv}")
        print(f"[SUCCESS] Labels saved to {args.labels_csv}")

    args:
      - --train_csv
      - {outputPath: train_csv}
      - --test_csv
      - {outputPath: test_csv}
      - --labels_csv
      - {outputPath: labels_csv}
